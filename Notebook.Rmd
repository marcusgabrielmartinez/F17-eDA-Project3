---
title: "Prediction of Parkinson's Disease"
author: 'Group 2: Kelly Jennings, Stefanos Kapetanakis, Marcus Martinez, Rachel Tarrant, Changyong Yi'
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
library(ggplot2)
require(dplyr)
require(ISLR)
require(boot)
library(tidyverse)
library(modelr)
require(MASS)
library(leaps)
library(glmnet)
require(data.world)
require(shiny)
knitr::opts_chunk$set(echo = TRUE)
```
  
## **R Session Info**  

```{r}
sessionInfo()
```

## **Github Link** 
https://github.com/marcusgabrielmartinez/F17-eDA-Project3

## **Connecting to data.world** 
```{r}
project <- "https://data.world/tarrantrl/parkinsons-telemonitoring"
df <- data.world::query(
  data.world::qry_sql("SELECT * FROM parkinsons_telemonitoring"),
  dataset = project
)
```

## Setup
We split our dataset into two for training and testing.
We also removed the subject id column and added a column to the dataset to translate the boolean strings for sex to binary values.

```{r}
df = dplyr::select(df, -subject)
df <- df %>% dplyr::mutate(sex2 = ifelse(sex == "true", 1, 0))
attach(df)
set.seed(1)
train = sample(nrow(df), 97)
test = df[-train,]
```


## **Introduction** 
"This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson's disease recruited to a six-month trial of a telemonitoring device for remote symptom progression monitoring. The recordings were automatically captured in the patient's homes.

Columns in the table contain subject number, subject age, subject gender, time interval from baseline recruitment date, motor UPDRS, total UPDRS, and 16 biomedical voice measures. Each row corresponds to one of 5,875 voice recording from these individuals. The main aim of the data is to predict the motor and total UPDRS scores ('motor_UPDRS' and 'total_UPDRS') from the 16 voice measures."


subject# - Integer that uniquely identifies each subject 
age - Subject age 
sex - Subject gender '0' - male, '1' - female 
test_time - Time since recruitment into the trial. The integer part is the number of days since recruitment. 
motor_UPDRS - Clinician's motor UPDRS score, linearly interpolated 
total_UPDRS - Clinician's total UPDRS score, linearly interpolated (UPDRS is stands for Unified Parkison's Disease Rating Scale and is used to rate the severity of Parkinson's)
Jitter(%),Jitter(Abs),Jitter:RAP,Jitter:PPQ5,Jitter:DDP - Several measures of variation in fundamental frequency (fundamental frequency is the lowest frequency of a sound wave).
Shimmer,Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,Shimmer:APQ11,Shimmer:DDA - Several measures of variation in amplitude 
NHR,HNR - Two measures of ratio of noise to tonal components in the voice 
RPDE - A nonlinear dynamical complexity measure 
DFA - Signal fractal scaling exponent 
PPE - A nonlinear measure of fundamental frequency variation

This document will compare models based on the predictors chosen by best subset selection, forward selection, and backward selection. These models include Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and K-Nearest Neighbors.

## Attempting to Predict Parkinson's Severity
First, we used forward subset selection to find the best model for predicting total UPDRS. In order to do this, we removed motor UPDRS from the data frame because it is a similar outcome measurement to total UPDRS.
 

```{r}
dftotal = dplyr::select(df, -motor_updrs)
regfit.fwd = regsubsets(total_updrs~., data=dftotal, nvmax=19, method="forward")
regfit.summary = summary(regfit.fwd)
regfit.summary
```


Here are the plots of both Cp and adjusted R2 from the models given from forward subset selection. 

```{r}
renderPlot(plot(regfit.fwd,scale="Cp"))
renderPlot(plot(regfit.summary$cp,xlab="Number of Variables",ylab="Cp", type="b"))
which.min(regfit.summary$cp)
renderPlot(plot(regfit.fwd,scale="adjr2"))
renderPlot(plot(regfit.summary$adjr2,xlab="Number of Variables",ylab="Adjr2", type="b"))
which.max(regfit.summary$adjr2)
```

The R2 values for all the models are very low. After considering these results in the context of the data, we realized that this is reasonable because all the patients have early Parkinson's disease. Perhaps it is difficult to predict the severiy of the disease because none of the patients have severe forms of the disease yet. Since the R2 values are so low for all the models, we decided to move on to predicting other outcomes with this data.

Find relevant insights here:
[Forward selection](https://data.world/tarrantrl/f-17-eda-project-3/insights/d3b42b42-4226-4b07-86b3-0c39bfddfe2b)
[Low R2](https://data.world/tarrantrl/f-17-eda-project-3/insights/421ff100-fd46-477d-9517-0b874376e721)


## Subset Selection Predicting Sex

Next, we used best subset selection to predict sex. We removed the original sex column in order to predict the binary version.


```{r}
df_subset_fixed <- df %>% dplyr::select(., -sex)

regfit.full=regsubsets(sex2~.,data=df_subset_fixed, nvmax=25)
reg.summary=summary(regfit.full)
reg.summary

```


The following plots show Cp and R2 for each model chosen by subset selection. Although the minimum Cp is at 15, the Cp levels off around 10 predictors. For this reason, we chose to do further analysis on the first 10 predictors chosen by subset selection.

```{r}
renderPlot(plot(regfit.full,scale="Cp"))
renderPlot(plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp"))
which.min(reg.summary$cp)

renderPlot(plot(regfit.full,scale="adjr2"))
renderPlot(plot(reg.summary$adjr2,xlab="Number of Variables",ylab="adjr2"))
which.max(reg.summary$adjr2)
```

We compared the predictors chosen by best subset selection to those chosen by forward selection. Forward selection chose the same predictors as best subset selection. 

```{r}
regfit.fwd=regsubsets(sex2~.,data=df_subset_fixed,nvmax=22,method="forward")
summary(regfit.fwd)
regfwd.summary=summary(regfit.fwd)

renderPlot(plot(regfit.fwd,scale="Cp"))
renderPlot(plot(regfwd.summary$cp,xlab="Number of Variables",ylab="Cp"))
which.min(regfwd.summary$cp)

renderPlot(plot(regfit.fwd,scale="adjr2"))
renderPlot(plot(regfwd.summary$adjr2,xlab="Number of Variables",ylab="adjr2"))
which.max(regfwd.summary$adjr2)
```

Find relevant insights here:
Initially, we did not exclude the string sex variable in subset selection. 
[Best Subset Selection, Predicting Sex](https://data.world/tarrantrl/f-17-eda-project-3/insights/4837162b-be8f-49b2-ae7e-1b28166ccb6d)
[Forward Selection Predicting Sex & Comparison to Best Subset](https://data.world/tarrantrl/f-17-eda-project-3/insights/703f3575-6e53-439c-88ab-5f6997d5a78e)
However, we realized the mistake and reran the insights.
[Correction: Best Subset & Forward Selection Predicting Sex](https://data.world/tarrantrl/f-17-eda-project-3/insights/cb333a27-3764-40bf-b665-a2321f85a61d)


## Using the Predictors from Subset Selection in Other Models
Even with exhaustive search, the adjusted R2 for the chosen models was still low (around 0.25). We decided to use the predictors from the ten predictor model that subset selection chose (as referenced above) in different model types. Specifically, we explored Logistic Regression (which is the model that subset selection uses), Linear Discriminate Analysis, Quadratic Discriminate Analysis, and K Nearest Neighbors. In order to compare these model types, we compared prediction accuracy.

### Logistic Regression
```{r}
renderPlot(plot(lda3.fit))
renderPlot({ggplot(lda3_df) + geom_histogram(mapping=aes(x=LD1))})
renderPlot({ggplot(lda3_df) + geom_boxplot(mapping = aes(x=class,y=LD1))})
```




## Quadratic Discriminant Analysis


Here we are analyzing two predictors, average hertz and jitter, to predict the Parkinson's Disease status of the patient. We are using Quadratic Discriminant Analysis to analyze the predictability of having Parkinson's Disease given these variables. 

```{r}
qda1.fit = qda(status ~ mdvp_fo_hz + mdvp_jitter,
              data=df, subset=train)
qda1.fit
qda1.pred = predict(qda1.fit, test)
table(qda1.pred$class,test$status)
table(qda1.pred$class==test$status)
table(qda1.pred$class!=test$status)
mean(qda1.pred$class==test$status)
```


Here we are analyzing six predictors, average hertz, jitter, rpde, d2, dfa, and spread1 to predict the Parkinson's Disease status of the patient. We are using Quadratic Discriminant Analysis to analyze the predictability of having Parkinson's Disease given these variables.  

```{r}
qda.fit3 = qda(status ~ mdvp_fo_hz + mdvp_jitter + rpde + d2 + dfa + spread1,
               data=df, subset=train)
qda.fit3
qda.pred3 = predict(qda.fit3, test)
table(qda.pred3$class,test$status)
table(qda.pred3$class==test$status)
table(qda.pred3$class!=test$status)
mean(qda.pred3$class==test$status)
```


## K-Nearest Neighbors

Here we are analyzing two predictors, average hertz and jitter, to predict the Parkinson's Disease status of the patient. We are using K-Nearest Neighbors to analyze the predictability of having Parkinson's Disease given these variables.

```{r}
test_knn = sample(nrow(test), 97)
predictors1=cbind(mdvp_fo_hz, mdvp_jitter)
knn1.pred=class::knn(predictors1[train, ],predictors1[test_knn,],status[train],k=1)
table(knn1.pred,status[test_knn])
mean(knn1.pred==status[test_knn])

```


Here we are analyzing six predictors, average hertz, jitter, rpde, d2, dfa, and spread1 to predict the Parkinson's Disease status of the patient. We are using K-Nearest Neighbors to analyze the predictability of having Parkinson's Disease given these variables. 

```{r}
predictors3=cbind(mdvp_fo_hz, mdvp_jitter, rpde, d2, dfa, spread1)
knn3.pred=class::knn(predictors3[train, ],predictors3[test_knn,],status[train],k=1)
table(knn3.pred,status[test_knn])
mean(knn3.pred==status[test_knn])

```


## Comparison of the Mean Correct Predictions


The following is a list of all the mean correct predictions for each of the models. For each type of analysis, the first mean represents the two predictor models and the second represents the six predictor models.

Logictic Regressions Results
```{r}
mean(glm1.pred==status2.test)
mean(glm3.pred==status2.test)

```

Linear Discriminant Analysis Results
```{r}
mean(lda1.pred$class==test$status)
mean(lda3.pred$class==test$status)
```

Quadratic Discriminant Analysis Results
```{r}
mean(qda1.pred$class==test$status)
mean(qda.pred3$class==test$status)
```

K-Nearest Neighbors Results
```{r}
mean(knn1.pred==status[test_knn])
mean(knn3.pred==status[test_knn])
```




## Research Progression

Using the pairs function we found predictors that aren't confounding and chose these to analyze. Initially we decided to choose average hertz and jitter as our predictors, because they yielded the best results among the two predictor combinations. Many of the variables were heavily correlated. For example, the fundamental frequency variables, the variation fundamental frequency variables, and the variation amplitude variables were all examples of this problem. Choosing one from each category gave us the variables with which we worked. We did further exploration, chose one variable each from jitter and shimmer, and then chose to include jitter instead of shimmer.

```{r}
sdf = dplyr::select(df, - status, - name, - mdvp_jitter_abs, -mdvp_ppq, - mdvp_rap, - mdvp_shimmer_db, - shimmer_apq3, - shimmer_apq5, - shimmer_dda, - status2)
renderPlot(pairs(sdf))
```

This plot is a good example of where we started in terms of narrowing down our predictors. 

##Insights

When analyzing each model, the greater mean for each analysis type was not consistant (i.e. neither the two nor six predictor model proved to be better). However, despite these inconsistancies, the differences between each model within each analysis were small, and the six predictor models tended to have higher means than the corresponding two predictor models within the same analysis type. These inconsistancies were a consequence of random sampling, as well as our data being relatively small. Our worst models still yielded relatively good results usually above .75 for both two and six predictor models. For the purpose of consistancy in this data report, we included a seed.

Even though these inconsistancies were present, the K-Nearest Neighbors (KNN) analysis type yields the overall best results. In fact, the six predictor model within the KNN analysis typically had the highest mean. The usual mean percentage was around .91 to .96.

Parkinson's Disease is associated with a certain speech pattern; because our predictors are measurements of the patients' speech pattern, it is unsurprising that even our worst models had high prediction rates. 


