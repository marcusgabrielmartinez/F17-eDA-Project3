---
title: "Prediction of Parkinson's Disease"
author: 'Group 2: Kelly Jennings, Stefanos Kapetanakis, Marcus Martinez, Rachel Tarrant, Changyong Yi'
resource_files:
- .Renviron
output:
  html_notebook:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
---

```{r setup, include=FALSE}
library(ggplot2)
require(dplyr)
require(ISLR)
require(boot)
library(tidyverse)
library(modelr)
require(MASS)
library(leaps)
library(glmnet)
require(data.world)
require(shiny)
knitr::opts_chunk$set(echo = TRUE)
```
  
## **R Session Info**  

```{r}
sessionInfo()
```

## **Github Link** 
https://github.com/marcusgabrielmartinez/F17-eDA-Project3

## **Connecting to data.world** 
```{r}
project <- "https://data.world/tarrantrl/parkinsons-telemonitoring"
df <- data.world::query(
  data.world::qry_sql("SELECT * FROM parkinsons_telemonitoring"),
  dataset = project
)
```

## Setup
We split our dataset into two for training and testing.
We also removed the subject id column and added a column to the dataset to translate the boolean strings for sex to binary values.

```{r}
df = dplyr::select(df, -subject)
df <- df %>% dplyr::mutate(sex2 = ifelse(sex == "true", 1, 0))
attach(df)
set.seed(1)
train = sample(nrow(df), 97)
test = df[-train,]
```


## **Introduction** 
"This dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson's disease recruited to a six-month trial of a telemonitoring device for remote symptom progression monitoring. The recordings were automatically captured in the patient's homes.

Columns in the table contain subject number, subject age, subject gender, time interval from baseline recruitment date, motor UPDRS, total UPDRS, and 16 biomedical voice measures. Each row corresponds to one of 5,875 voice recording from these individuals. The main aim of the data is to predict the motor and total UPDRS scores ('motor_UPDRS' and 'total_UPDRS') from the 16 voice measures."


subject# - Integer that uniquely identifies each subject 
age - Subject age 
sex - Subject gender '0' - male, '1' - female 
test_time - Time since recruitment into the trial. The integer part is the number of days since recruitment. 
motor_UPDRS - Clinician's motor UPDRS score, linearly interpolated 
total_UPDRS - Clinician's total UPDRS score, linearly interpolated (UPDRS is stands for Unified Parkison's Disease Rating Scale and is used to rate the severity of Parkinson's)
Jitter(%),Jitter(Abs),Jitter:RAP,Jitter:PPQ5,Jitter:DDP - Several measures of variation in fundamental frequency (fundamental frequency is the lowest frequency of a sound wave).
Shimmer,Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,Shimmer:APQ11,Shimmer:DDA - Several measures of variation in amplitude 
NHR,HNR - Two measures of ratio of noise to tonal components in the voice 
RPDE - A nonlinear dynamical complexity measure 
DFA - Signal fractal scaling exponent 
PPE - A nonlinear measure of fundamental frequency variation

This document will compare models based on the predictors chosen by best subset selection, forward selection, and backward selection. These models include Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and K-Nearest Neighbors.

## Attempting to Predict Parkinson's Severity
First, we used forward subset selection to find the best model for predicting total UPDRS. In order to do this, we removed motor UPDRS from the data frame because it is a similar outcome measurement to total UPDRS.
 

```{r}
dftotal = dplyr::select(df, -motor_updrs)
regfit.fwd = regsubsets(total_updrs~., data=dftotal, nvmax=19, method="forward")
regfit.summary = summary(regfit.fwd)
regfit.summary
```


Here are the plots of both Cp and adjusted R2 from the models given from forward subset selection. 

```{r}
renderPlot(plot(regfit.fwd,scale="Cp"))
renderPlot(plot(regfit.summary$cp,xlab="Number of Variables",ylab="Cp", type="b"))
which.min(regfit.summary$cp)
renderPlot(plot(regfit.fwd,scale="adjr2"))
renderPlot(plot(regfit.summary$adjr2,xlab="Number of Variables",ylab="Adjr2", type="b"))
which.max(regfit.summary$adjr2)
```

The R2 values for all the models are very low. After considering these results in the context of the data, we realized that this is reasonable because all the patients have early Parkinson's disease. Perhaps it is difficult to predict the severiy of the disease because none of the patients have severe forms of the disease yet. Since the R2 values are so low for all the models, we decided to move on to predicting other outcomes with this data.

Find relevant insights here:
[Forward selection](https://data.world/tarrantrl/f-17-eda-project-3/insights/d3b42b42-4226-4b07-86b3-0c39bfddfe2b)
[Low R2](https://data.world/tarrantrl/f-17-eda-project-3/insights/421ff100-fd46-477d-9517-0b874376e721)


## Subset Selection Predicting Sex

Next, we used best subset selection to predict sex. We removed the original sex column in order to predict the binary version.


```{r}
df_subset_fixed <- df %>% dplyr::select(., -sex)

regfit.full=regsubsets(sex2~.,data=df_subset_fixed, nvmax=25)
reg.summary=summary(regfit.full)
reg.summary

```


The following plots show Cp and R2 for each model chosen by subset selection. Although the minimum Cp is at 15, the Cp levels off around 10 predictors. For this reason, we chose to do further analysis on the first 10 predictors chosen by subset selection.

```{r}
renderPlot(plot(regfit.full,scale="Cp"))
renderPlot(plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp"))
which.min(reg.summary$cp)

renderPlot(plot(regfit.full,scale="adjr2"))
renderPlot(plot(reg.summary$adjr2,xlab="Number of Variables",ylab="adjr2"))
which.max(reg.summary$adjr2)
```

We compared the predictors chosen by best subset selection to those chosen by forward selection. Forward selection chose the same predictors as best subset selection. 

```{r}
regfit.fwd=regsubsets(sex2~.,data=df_subset_fixed,nvmax=22,method="forward")
summary(regfit.fwd)
regfwd.summary=summary(regfit.fwd)

renderPlot(plot(regfit.fwd,scale="Cp"))
renderPlot(plot(regfwd.summary$cp,xlab="Number of Variables",ylab="Cp"))
which.min(regfwd.summary$cp)

renderPlot(plot(regfit.fwd,scale="adjr2"))
renderPlot(plot(regfwd.summary$adjr2,xlab="Number of Variables",ylab="adjr2"))
which.max(regfwd.summary$adjr2)
```

Find relevant insights here:
Initially, we did not exclude the string sex variable in subset selection. 
[Best Subset Selection, Predicting Sex](https://data.world/tarrantrl/f-17-eda-project-3/insights/4837162b-be8f-49b2-ae7e-1b28166ccb6d)
[Forward Selection Predicting Sex & Comparison to Best Subset](https://data.world/tarrantrl/f-17-eda-project-3/insights/703f3575-6e53-439c-88ab-5f6997d5a78e)
However, we realized the mistake and reran the insights.
[Correction: Best Subset & Forward Selection Predicting Sex](https://data.world/tarrantrl/f-17-eda-project-3/insights/cb333a27-3764-40bf-b665-a2321f85a61d)


## Using the Predictors from Subset Selection in Other Models
Even with exhaustive search, the adjusted R2 for the chosen models was still low (around 0.25). We decided to use the predictors from the ten predictor model that subset selection chose (as referenced above) in different model types. Specifically, we explored Logistic Regression (which is the model that subset selection uses), Linear Discriminate Analysis, Quadratic Discriminate Analysis, and K Nearest Neighbors. In order to compare these model types, we compared prediction accuracy.

### Logistic Regression
First, we did Logistic Regression. This is the least-squares model that subset selection also uses.
```{r}
set.seed(1)
train = sample(nrow(df_subset_fixed), 2937)
test = df_subset_fixed[-train,]

## WHY ISN'T THIS WORKING
glm2.fit=glm(sex2 ~ motor_updrs + total_updrs + jitter_abs + jitter_rap + jitter_ppq5 + shimmer_apq11 + nhr + hnr + rpde + ppe,
             data=df, family=binomial,
             subset=train)
summary(glm2.fit)

glm2.probs=predict(glm2.fit,newdata=test,type="response")
glm2.pred=ifelse(glm2.probs>0.5,"1","0")
sex2.test = test$sex2
table(glm2.pred,sex2.test) 
mean(glm2.pred==sex2.test) 
```

### Linear Discriminant Analysis

Next, we did Linear Discriminant Analysis with the same predictors. 

```{r}
lda2.fit=lda(sex2 ~ motor_updrs + total_updrs + jitter_rap + jitter_ppq5 + shimmer_apq11 + nhr + hnr + rpde + ppe,
             data=df, subset=train)
lda2.fit
lda2.pred=predict(lda2.fit, test)
lda2_df = data.frame(lda2.pred)
table(lda2.pred$class,test$sex2)
mean(lda2.pred$class==test$sex2)
```


### Quadratic Discriminant Analysis

Next, we did Quadratic Discriminant Analysis with the same predictors.

```{r}
qda2.fit = qda(sex2 ~ motor_updrs + total_updrs + jitter_abs + jitter_rap + jitter_ppq5 + shimmer_apq11 + nhr + hnr + rpde + ppe,
               data=df, subset=train)
qda2.fit
qda2.pred = predict(qda2.fit, test)
table(qda2.pred$class,test$sex2)
mean(qda2.pred$class==test$sex2)
```


### K-Nearest Neighbors

Finally, we did K-Nearest Neighbors with the 10 predictors.

```{r}
test_knn = sample(nrow(test), 2937)
predictorsKNN=cbind(motor_updrs, total_updrs, jitter_abs, jitter_rap, jitter_ppq5, shimmer_apq11, nhr, hnr, rpde, ppe)
knn.pred=class::knn(predictorsKNN[train, ],predictorsKNN[test_knn,],sex2[train],k=1)
table(knn.pred,sex2[test_knn])
mean(knn.pred==sex2[test_knn])

```

Here are relevant insights:
[KNN Analysis of New "Best Model"](https://data.world/tarrantrl/f-17-eda-project-3/insights/cdc5411d-6061-4671-89b3-f3f24ddee86b)
[Testing if KNN Analysis is the best with New Predictors](https://data.world/tarrantrl/f-17-eda-project-3/insights/9a5567f6-da54-4c88-ad7c-604d61c3b504)


### Comparing Prediction Means

After doing these analyses, we found that KNN gives the best prediction rate of sex. In fact, the prediction rate is nearly 100%. We found it interesting that we could predict sex with such a high degree of accuracy based on these measures of Parkinson's disease. This led us to tentatively conclude that these aspects of Parkinson's are sexually dimorphic.

```{r}
mean(glm2.pred==sex2.test) 
mean(lda2.pred$class==test$sex2)
mean(qda2.pred$class==test$sex2)
mean(knn.pred==sex2[test_knn])
```




## Insights

Ultimately, we were able to use the predictors found by subset selection to find a very accurate model. Even though subset selection did not give a very high R2 value with any of its models, it still gave us valuable insight into the data as to which predictors to include in other classification models.

